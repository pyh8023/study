Trino根据生成的逻辑执行计划将其拆分成多个具有层级关系的Stage，每个Stage会进一步分解为若干个Task，并将Task调度到不同的Worker节点上执行，每个的Stage有不同的调度策略。

查询调度的入口在SqlQueryExecution.start()中，根据不同的重试策略选择QueryScheduler实现类，然后调用QueryScheduler的start()方法启动查询调度。（本文针对trino的405版本源码进行分析）

当重试策略为NONE或QUERY时，采用PipelinedQueryScheduler进行调度，重试策略为TASK时，采用EventDrivenFaultTolerantQueryScheduler或FaultTolerantQueryScheduler，其中FaultTolerantQueryScheduler已过时，被EventDrivenFaultTolerantQueryScheduler替代，因此下面主要对PipelinedQueryScheduler和EventDrivenFaultTolerantQueryScheduler进行分析。



# QueryScheduler

## PipelinedQueryScheduler
当重试策略为NONE或QUERY时，PipelinedQueryScheduler负责查询调度。
PipelinedQueryScheduler的大致调度过程如下图所示：

![image-pipelinedQueryScheduler](../trino/images/image-pipelinedQueryScheduler.png)

在PipelinedQueryScheduler中创建PipelinedQueryScheduler.CoordinatorStagesScheduler 和PipelinedQueryScheduler.DistributedStagesScheduler两个Stage调度器：

PipelinedQueryScheduler.CoordinatorStagesScheduler负责Coordinator_Only类型的Stage调度

PipelinedQueryScheduler.DistributedStagesScheduler负责Coordinator_Only类型之外的所有Stage的调度。



PipelinedQueryScheduler的构造方法调用详细流程如下：

1.遍历逻辑执行计划中所有的SubPlan，每个SubPlan创建一个SqlStage，负责跟踪来自同一stage的任务的执行统计信息，在执行完成时提供最终stage信息。

2.将Stage分成两类，Coordinator_only类型的Stage存放到coordinatorStagesInTopologicalOrder中，其他类型存放到distributedStagesInTopologicalOrder中。

3.创建CoordinatorStagesScheduler

![PipelinedQueryScheduler_create](../trino/images/PipelinedQueryScheduler_create.png)


### CoordinatorStagesScheduler

在PipelinedQueryScheduler的构造方法中会调用CoordinatorStagesScheduler.create()，create()方法的调用过程可参考PipelinedQueryScheduler构造方法执行图。

CoordinatorStagesScheduler的创建步骤如下：

1.为Coordinator消费的Stage创建的OutputBuffer：创建的OutputBuffer都存放到Map<PlanFragmentId, PipelinedOutputBufferManager>中，每个Stage对应一个PipelinedOutputBufferManager，由其负责对应Stage的OutputBuffer管理，先为Output Stage创建PipelinedOutputBufferManager，然后遍历所有coordinatorStagesInTopologicalOrder中的Stage，为其上游Stage创建PipelinedOutputBufferManager。上述创建的是PartitionedPipelinedOutputBufferManager。

2.为Coordinator消费的Stage创建的BucketToPartition：创建的BucketToPartition存放在Map<PlanFragmentId, Optional<int[]>>中，先添加SINGLE_PARTITION（SINGLE_PARTITION = new int[] {0}）作为Output Stage的BucketToPartition，然后遍历所有coordinatorStagesInTopologicalOrder中的Stage，为上游Stage添加BucketToPartition，同样添加SINGLE_PARTITION作为BucketToPartition。

3.遍历上一步coordinatorStagesInTopologicalOrder中的所有Coordinator_only类型的Stage，为每个Stage创建PipelinedStageExecution，通过PipelinedStageExecution将流水线中的stage连接在一起，添加任务时通知上游stage添加output buffer，下游stage更新source task列表。

4.创建CoordinatorStagesScheduler，并为上一步创建的所有PipelinedStageExecution绑定事件变更事件。

当调用PipelinedQueryScheduler的start()启动查询调度时，会调用CoordinatorStagesScheduler.schedule()，直接选择coordinator节点运行Coordinator_Only类型的Stage的任务。

```java
 public synchronized void schedule()
        {
            if (!scheduled.compareAndSet(false, true)) {
                return;
            }
            TaskFailureReporter failureReporter = new TaskFailureReporter(distributedStagesScheduler);
            queryStateMachine.addOutputTaskFailureListener(failureReporter);

            //选择当前coordinator运行任务
            InternalNode coordinator = nodeScheduler.createNodeSelector(queryStateMachine.getSession(), Optional.empty()).selectCurrentNode();
            for (StageExecution stageExecution : stageExecutions) {
                //调度任务，创建HttpRemoteTask
                Optional<RemoteTask> remoteTask = stageExecution.scheduleTask(
                        coordinator,
                        0,
                        ImmutableMultimap.of());
                //更新stageExecution状态为调度完成
                stageExecution.schedulingComplete();
                //为任务绑定SourceTask失败监听器
                remoteTask.ifPresent(task -> coordinatorTaskManager.addSourceTaskFailureListener(task.getTaskId(), failureReporter));
                //如果查询状态为STARTING，且remoteTask已创建，则更新查询状态为Running
                if (queryStateMachine.getQueryState() == STARTING && remoteTask.isPresent()) {
                    queryStateMachine.transitionToRunning();
                }
            }
        }
```

### DistributedStagesScheduler
DistributedStagesScheduler负责Coordinator_Only类型之外的所有Stage的调度。

#### DistributedStagesScheduler创建

DistributedStagesScheduler的create()的执行流程如下所示：

![DistributedStagesScheduler_create](../trino/images/DistributedStagesScheduler_create.png)

其中PipelinedQueryScheduler.DistributedStagesScheduler.create()的源码如下：

```java
public static DistributedStagesScheduler create(
                QueryStateMachine queryStateMachine,
                SplitSchedulerStats schedulerStats,
                NodeScheduler nodeScheduler,
                NodePartitioningManager nodePartitioningManager,
                StageManager stageManager,
                CoordinatorStagesScheduler coordinatorStagesScheduler,
                ExecutionPolicy executionPolicy,
                FailureDetector failureDetector,
                ScheduledExecutorService executor,
                SplitSourceFactory splitSourceFactory,
                int splitBatchSize,
                DynamicFilterService dynamicFilterService,
                TableExecuteContextManager tableExecuteContextManager,
                RetryPolicy retryPolicy,
                int attempt)
        {
            DistributedStagesSchedulerStateMachine stateMachine = new DistributedStagesSchedulerStateMachine(queryStateMachine.getQueryId(), executor);

            Map<PartitioningHandle, NodePartitionMap> partitioningCacheMap = new HashMap<>();
            Function<PartitioningHandle, NodePartitionMap> partitioningCache = partitioningHandle ->
                    partitioningCacheMap.computeIfAbsent(partitioningHandle, handle -> nodePartitioningManager.getNodePartitioningMap(
                            queryStateMachine.getSession(),
                            handle.equals(SCALED_WRITER_HASH_DISTRIBUTION) ? FIXED_HASH_DISTRIBUTION : handle));

            //为每一个Stage创建bucketToPartition，先添加coordinatorStagesScheduler中创建的BucketToPartition，然后遍历distributedStagesInTopologicalOrder，为其上游Stage创建BucketToPartition
            Map<PlanFragmentId, Optional<int[]>> bucketToPartitionMap = createBucketToPartitionMap(
                    coordinatorStagesScheduler.getBucketToPartitionForStagesConsumedByCoordinator(),
                    stageManager,
                    partitioningCache);
  
            //为每一个Stage创建PipelinedOutputBufferManager，先添加coordinatorStagesScheduler中创建的PipelinedOutputBufferManager，然后遍历distributedStagesInTopologicalOrder，为其上游Stage创建PipelinedOutputBufferManager
	    //根据Stage对应PlanFragment的Partitioning类型，选择不同的PipelinedOutputBufferManager实现类
            //FIXED_BROADCAST_DISTRIBUTION类型选择BroadcastPipelinedOutputBufferManager
	    //SCALED_WRITER_ROUND_ROBIN_DISTRIBUTION选择ScaledPipelinedOutputBufferManager
	    //其他类型选择PartitionedPipelinedOutputBufferManager
            Map<PlanFragmentId, PipelinedOutputBufferManager> outputBufferManagers = createOutputBufferManagers(
                    coordinatorStagesScheduler.getOutputBuffersForStagesConsumedByCoordinator(),
                    stageManager,
                    bucketToPartitionMap);

            TaskLifecycleListener coordinatorTaskLifecycleListener = coordinatorStagesScheduler.getTaskLifecycleListener();
	    //当开启重启机制，仅在查询完成时关闭coordinator上的exchange clients
            if (retryPolicy != RetryPolicy.NONE) {
                TaskLifecycleListenerBridge taskLifecycleListenerBridge = new TaskLifecycleListenerBridge(coordinatorTaskLifecycleListener);
                coordinatorTaskLifecycleListener = taskLifecycleListenerBridge;
                stateMachine.addStateChangeListener(state -> {
                    if (state == DistributedStagesSchedulerState.FINISHED) {
                        taskLifecycleListenerBridge.notifyNoMoreSourceTasks();
                    }
                });
            }

            Map<StageId, StageExecution> stageExecutions = new HashMap<>();
            for (SqlStage stage : stageManager.getDistributedStagesInTopologicalOrder()) {
                Optional<SqlStage> parentStage = stageManager.getParent(stage.getStageId());
              
                //获取下游Stage的TaskLifecycleListener
                TaskLifecycleListener taskLifecycleListener;
                if (parentStage.isEmpty() || parentStage.get().getFragment().getPartitioning().isCoordinatorOnly()) {
                    // output will be consumed by coordinator
                    taskLifecycleListener = coordinatorTaskLifecycleListener;
                }
                else {
                    StageId parentStageId = parentStage.get().getStageId();
                    StageExecution parentStageExecution = requireNonNull(stageExecutions.get(parentStageId), () -> "execution is null for stage: " + parentStageId);
                    taskLifecycleListener = parentStageExecution.getTaskLifecycleListener();
                }

                PlanFragment fragment = stage.getFragment();
              
                //创建PipelinedStageExecution
                StageExecution stageExecution = createPipelinedStageExecution(
                        stageManager.get(fragment.getId()),
                        outputBufferManagers,
                        taskLifecycleListener,
                        failureDetector,
                        executor,
                        bucketToPartitionMap.get(fragment.getId()),
                        attempt);
                stageExecutions.put(stage.getStageId(), stageExecution);
            }

            ImmutableMap.Builder<StageId, StageScheduler> stageSchedulers = ImmutableMap.builder();
            for (StageExecution stageExecution : stageExecutions.values()) {
                List<StageExecution> children = stageManager.getChildren(stageExecution.getStageId()).stream()
                        .map(stage -> requireNonNull(stageExecutions.get(stage.getStageId()), () -> "stage execution not found for stage: " + stage))
                        .collect(toImmutableList());
              
                //创建StageScheduler
                StageScheduler scheduler = createStageScheduler(
                        queryStateMachine,
                        stageExecution,
                        splitSourceFactory,
                        children,
                        partitioningCache,
                        nodeScheduler,
                        nodePartitioningManager,
                        splitBatchSize,
                        dynamicFilterService,
                        executor,
                        tableExecuteContextManager);
                stageSchedulers.put(stageExecution.getStageId(), scheduler);
            }

            DistributedStagesScheduler distributedStagesScheduler = new DistributedStagesScheduler(
                    stateMachine,
                    queryStateMachine,
                    schedulerStats,
                    stageManager,
                    executionPolicy.createExecutionSchedule(stageExecutions.values()),
                    stageSchedulers.buildOrThrow(),
                    ImmutableMap.copyOf(stageExecutions),
                    dynamicFilterService);
  
            //为每个StageExecution设置状态变更监听器
            distributedStagesScheduler.initialize();
            return distributedStagesScheduler;
        }

```

createStageScheduler()方法负责StageScheduler的创建，会根据不同类型的Stage选择不同的StageScheduler实现类。

StageScheduler的实现类有：

1.包装了SourcePartitionedScheduler的匿名类

2.FixedSourcePartitionedScheduler

3.FixedCountScheduler

4.ScaledWriterScheduler

createStageScheduler()创建StageScheduler的源码如下：

```java
        private static StageScheduler createStageScheduler(
                QueryStateMachine queryStateMachine,
                StageExecution stageExecution,
                SplitSourceFactory splitSourceFactory,
                List<StageExecution> childStageExecutions,
                Function<PartitioningHandle, NodePartitionMap> partitioningCache,
                NodeScheduler nodeScheduler,
                NodePartitioningManager nodePartitioningManager,
                int splitBatchSize,
                DynamicFilterService dynamicFilterService,
                ScheduledExecutorService executor,
                TableExecuteContextManager tableExecuteContextManager)
        {
            Session session = queryStateMachine.getSession();
            PlanFragment fragment = stageExecution.getFragment();
          
            //获取PlanFragment的Partitioning类型
            PartitioningHandle partitioningHandle = fragment.getPartitioning();
            //只有Source Stage(需要从数据源表查询数据)，splitSources不为空
            Map<PlanNodeId, SplitSource> splitSources = splitSourceFactory.createSplitSources(session, fragment);
            
            ...

            //当partitioning为SOURCE_DISTRIBUTION类型时，创建SourcePartitionedScheduler
            if (partitioningHandle.equals(SOURCE_DISTRIBUTION)) {
                // nodes are selected dynamically based on the constraints of the splits and the system load
                Entry<PlanNodeId, SplitSource> entry = getOnlyElement(splitSources.entrySet());
                PlanNodeId planNodeId = entry.getKey();
                SplitSource splitSource = entry.getValue();
                Optional<CatalogHandle> catalogHandle = Optional.of(splitSource.getCatalogHandle())
                        .filter(catalog -> !catalog.getType().isInternal());
                NodeSelector nodeSelector = nodeScheduler.createNodeSelector(session, catalogHandle);
		//创建动态Split放置策略
                SplitPlacementPolicy placementPolicy = new DynamicSplitPlacementPolicy(nodeSelector, stageExecution::getAllTasks);

                return newSourcePartitionedSchedulerAsStageScheduler(
                        stageExecution,
                        planNodeId,
                        splitSource,
                        placementPolicy,
                        splitBatchSize,
                        dynamicFilterService,
                        tableExecuteContextManager,
                        () -> childStageExecutions.stream().anyMatch(StageExecution::isAnyTaskBlocked));
            }

            //当partitioning为SCALED_WRITER_ROUND_ROBIN_DISTRIBUTION类型时，创建ScaledWriterScheduler
            if (partitioningHandle.equals(SCALED_WRITER_ROUND_ROBIN_DISTRIBUTION)) {
                Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStageExecutions.stream()
                        .map(StageExecution::getTaskStatuses)
                        .flatMap(List::stream)
                        .collect(toImmutableList());
                Supplier<Collection<TaskStatus>> writerTasksProvider = stageExecution::getTaskStatuses;

                ScaledWriterScheduler scheduler = new ScaledWriterScheduler(
                        stageExecution,
                        sourceTasksProvider,
                        writerTasksProvider,
                        nodeScheduler.createNodeSelector(session, Optional.empty()),
                        executor,
                        getWriterMinSize(session),
                        isTaskScaleWritersEnabled(session) ? getTaskScaleWritersMaxWriterCount(session) : getTaskWriterCount(session));

                whenAllStages(childStageExecutions, StageExecution.State::isDone)
                        .addListener(scheduler::finish, directExecutor());

                return scheduler;
            }

            //如果不是Source Stage，则创建FixedCountScheduler
            if (splitSources.isEmpty()) {
                // all sources are remote
                //实际调用nodePartitioningManager.getNodePartitioningMap()
                //partitioning为SINGLE时，随机选择一个节点加入到NodePartitionMap中
                //partitioning为FIXED时，随机选择多个节点，数量不超过hash_partition_count
                //partitioning为COORDINATOR_ONLY时，选择Coordinator节点加入
                NodePartitionMap nodePartitionMap = partitioningCache.apply(partitioningHandle);
                List<InternalNode> partitionToNode = nodePartitionMap.getPartitionToNode();
                checkCondition(!partitionToNode.isEmpty(), NO_NODES_AVAILABLE, "No worker nodes available");
                return new FixedCountScheduler(stageExecution, partitionToNode);
            }

            //存储 local source PlanNodeId
            List<PlanNodeId> schedulingOrder = fragment.getPartitionedSources();
            Optional<CatalogHandle> catalogHandle = partitioningHandle.getCatalogHandle();
            checkArgument(catalogHandle.isPresent(), "No catalog handle for partitioning handle: %s", partitioningHandle);

            BucketNodeMap bucketNodeMap;
            List<InternalNode> stageNodeList;
          
            //创建BucketNodeMap
            if (fragment.getRemoteSourceNodes().stream().allMatch(node -> node.getExchangeType() == REPLICATE)) {
                //没有RemoteSourceNode或者所有RemoteSourceNode的ExchangeType为REPLICATE
                //由对应的Connector提供的ConnectorNodePartitioningProvider提供bucketNodeMap
                bucketNodeMap = nodePartitioningManager.getBucketNodeMap(session, partitioningHandle);
                //获取所有节点
                stageNodeList = new ArrayList<>(nodeScheduler.createNodeSelector(session, catalogHandle).allNodes());
                Collections.shuffle(stageNodeList);
            }
            else {
                // remote source requires nodePartitionMap
                //实际调用nodePartitioningManager.getNodePartitioningMap()
                //partitioning为SINGLE时，随机选择一个节点加入到NodePartitionMap中
                //partitioning为FIXED时，随机选择多个节点，数量不超过hash_partition_count
                //partitioning为COORDINATOR_ONLY时，选择Coordinator节点加入
                NodePartitionMap nodePartitionMap = partitioningCache.apply(partitioningHandle);
                stageNodeList = nodePartitionMap.getPartitionToNode();
                bucketNodeMap = nodePartitionMap.asBucketNodeMap();
            }

            //FixedSourcePartitionedScheduler构造方法中会为每一个local source创建SourcePartitionedScheduler
            //SplitPlacementPolicy为BucketedSplitPlacementPolicy
            return new FixedSourcePartitionedScheduler(
                    stageExecution,
                    splitSources,
                    schedulingOrder,
                    stageNodeList,
                    bucketNodeMap,
                    splitBatchSize,
                    nodeScheduler.createNodeSelector(session, catalogHandle),
                    dynamicFilterService,
                    tableExecuteContextManager);
        }

```

#### DistributedStagesScheduler调度
DistributedStagesScheduler.schedule()执行步骤如下：

1.调用所有StageScheduler的start()，只有SourcePartitionedScheduler.start()会执行相关初始化操作，其他StageScheduler的start()内容为空，不做任何操作。

2.当执行调度没有完成时，会循环执行后续步骤。

3.获取未调度的StageExecution的集合，调用每个StageExecution对应的StageScheduler的schedule()，进行Stage调度，并返回调度结果。

4.根据调度结果更新状态。

5.处理阻塞的Stage。

```java
       public void schedule()
        {
            checkState(started.compareAndSet(false, true), "already started");

            try (SetThreadName ignored = new SetThreadName("Query-%s", queryStateMachine.getQueryId())) {
                //调用所有StageScheduler的start()
                stageSchedulers.values().forEach(StageScheduler::start);
              
                while (!executionSchedule.isFinished()) {
                    List<ListenableFuture<Void>> blockedStages = new ArrayList<>();
                    //获取未调度的StageExecution的集合
                    StagesScheduleResult stagesScheduleResult = executionSchedule.getStagesToSchedule();
                    for (StageExecution stageExecution : stagesScheduleResult.getStagesToSchedule()) {
                        stageExecution.beginScheduling();

                        //执行调度
                        ScheduleResult result = stageSchedulers.get(stageExecution.getStageId())
                                .schedule();

                        ...
                        }
                    }

                    ...
                    //处理阻塞的Stage
                }

                ...
            }
            catch (Throwable t) {
                fail(t, Optional.empty());
            }
            finally {
                ...
                //关闭StageScheduler
            }
        }

```

查询有3种执行策略，分别为phased、all-at-once和legacy-phased，可通过query.execution-policy配置指定，默认的执行策略为phased。

这3种执行策略对应了ExecutionSchedule的三个实现类：

1.PhasedExecutionSchedule ：按顺序调度各Stage，避免由于Stage间的依赖性造成阻塞，此策略最大限度地提高集群资源的利用率。

2.AllAtOnceExecutionSchedule：一次性调度所有Stage。因此，集群资源的利用率最初很高，但是Stage间的依赖性通常会导致等待时间更长，从而增加整个查询阻塞时间。

3.LegacyPhasedExecutionSchedule ：具有与PhasedExecutionSchedule相似的功能，但该策略会维持Stage运行数量最小化，从而增加查询阻塞时间。

### PipelinedStageExecution
PipelinedStageExecution负责将流水线中的stage连接在一起，添加任务时通知上游stage添加output buffer，下游stage更新source task列表，以及stage执行状态转换。

#### PipelinedStageExecution创建
通过调用PipelinedStageExecution的createPipelinedStageExecution()创建PipelinedStageExecution，对应的源码如下：

```java
    public static PipelinedStageExecution createPipelinedStageExecution(
            SqlStage stage,
            Map<PlanFragmentId, PipelinedOutputBufferManager> outputBufferManagers,
            TaskLifecycleListener taskLifecycleListener,
            FailureDetector failureDetector,
            Executor executor,
            Optional<int[]> bucketToPartition,
            int attempt)
    {
        //创建Stage状态器
        PipelinedStageStateMachine stateMachine = new PipelinedStageStateMachine(stage.getStageId(), executor);
        
        //遍历当前Stage的所有RemoteSourceNode，将RemoteSourceNode对应的上游Stage的planFragmentId映射添加到exchangeSources中
        ImmutableMap.Builder<PlanFragmentId, RemoteSourceNode> exchangeSources = ImmutableMap.builder();
        for (RemoteSourceNode remoteSourceNode : stage.getFragment().getRemoteSourceNodes()) {
            for (PlanFragmentId planFragmentId : remoteSourceNode.getSourceFragmentIds()) {
                exchangeSources.put(planFragmentId, remoteSourceNode);
            }
        }
        
        PipelinedStageExecution execution = new PipelinedStageExecution(
                stateMachine,
                stage,
                outputBufferManagers,
                taskLifecycleListener,
                failureDetector,
                bucketToPartition,
                exchangeSources.buildOrThrow(),
                attempt);
        
        //为StageExecution设置状态变更监听器
        execution.initialize();
        return execution;
    }



    private void initialize()
    {
        stateMachine.addStateChangeListener(state -> {
            if (!state.canScheduleMoreTasks()) {
                //通知下游Stage，当前Stage已无新增任务
                taskLifecycleListener.noMoreTasks(stage.getFragment().getId());
                //通知上游Stage的PipelinedOutputBufferManager已无更多OutputBuffer，并更新上游Task的OutputBuffer
                updateSourceTasksOutputBuffers(PipelinedOutputBufferManager::noMoreBuffers);
            }
        });
    }

    private synchronized void updateSourceTasksOutputBuffers(Consumer<PipelinedOutputBufferManager> updater)
    {

        for (PlanFragmentId sourceFragment : exchangeSources.keySet()) {
            PipelinedOutputBufferManager outputBufferManager = outputBufferManagers.get(sourceFragment);
            //更新outputBufferManager中的outputBuffer，添加outputBuffer或者通知PipelinedOutputBufferManager已无更多outputBuffer
            updater.accept(outputBufferManager);
            //更新所有上游Task的OutputBuffer
            for (RemoteTask sourceTask : sourceTasks.get(sourceFragment)) {
                sourceTask.setOutputBuffers(outputBufferManager.getOutputBuffers());
            }
        }
    }
```

 #### PipelinedStageExecution任务调度


```java
    public synchronized Optional<RemoteTask> scheduleTask(
            InternalNode node,
            int partition,
            Multimap<PlanNodeId, Split> initialSplits)
    {
        if (stateMachine.getState().isDone()) {
            return Optional.empty();
        }

        checkArgument(!tasks.containsKey(partition), "A task for partition %s already exists", partition);

        //获取当前Stage的outputBuffer
        OutputBuffers outputBuffers = outputBufferManagers.get(stage.getFragment().getId()).getOutputBuffers();

        //在Node上创建一个Task
        Optional<RemoteTask> optionalTask = stage.createTask(
                node,
                partition,
                attempt,
                bucketToPartition,
                outputBuffers,
                initialSplits,
                ImmutableSet.of(),
                Optional.empty());

        if (optionalTask.isEmpty()) {
            return Optional.empty();
        }

        RemoteTask task = optionalTask.get();

        tasks.put(partition, task);

        //遍历上游Stage中的所有Task,为每个Task创建ExchangeSplit,添加到exchangeSplits中
        ImmutableMultimap.Builder<PlanNodeId, Split> exchangeSplits = ImmutableMultimap.builder();
        sourceTasks.forEach((fragmentId, sourceTask) -> {
            TaskStatus status = sourceTask.getTaskStatus();
            if (status.getState() != TaskState.FINISHED) {
                //exchangeSources为当前Stage中的RemoteSourceNode
                PlanNodeId planNodeId = exchangeSources.get(fragmentId).getId();
                exchangeSplits.put(planNodeId, createExchangeSplit(sourceTask, task));
            }
        });

        allTasks.add(task.getTaskId());

        //将上游Task输出对应的exchangeSplits添加到当前Task的split列表中
        task.addSplits(exchangeSplits.build());
        completeSources.forEach(task::noMoreSplits);

        //为任务绑定状态变更监听器
        task.addStateChangeListener(this::updateTaskStatus);

        //启动任务
        task.start();

        //触发下游Stage的taskCreated事件，更新下游Stage的Source Task或更新查询结果
        taskLifecycleListener.taskCreated(stage.getFragment().getId(), task);

        //根据Task的PartitionId生成OutputBufferId
        OutputBufferId outputBufferId = new OutputBufferId(task.getTaskId().getPartitionId());
        //将生成的outputBufferId添加到上游Stage的outputBufferManager中，并更新上游Task的OutputBuffer
        updateSourceTasksOutputBuffers(outputBufferManager -> outputBufferManager.addOutputBuffer(outputBufferId));

        return Optional.of(task);
    }


```



#### TaskLifecycleListener
TaskLifecycleListener负责Task创建和Task执行结束事件处理，TaskLifecycleListener接口中有下面两个接口:

```java
public interface TaskLifecycleListener
{
    void taskCreated(PlanFragmentId fragmentId, RemoteTask task);

    void noMoreTasks(PlanFragmentId fragmentId);
}
```


TaskLifecycleListener有3个实现类：

1.PipelinedQueryScheduler.QueryOutputTaskLifecycleListener

负责更新查询结果的输入，最终的Output Stage会绑定这个TaskLifecycleListener

2.PipelinedStageExecution.getTaskLifecycleListener()中创建的匿名TaskLifecycleListener

当没有开启重启策略，且存在父Stage时，会绑定父Stage中创建的TaskLifecycleListener

3.PipelinedQueryScheduler.TaskLifecycleListenerBridge

当开启重启策略时使用该实现类，创建时需要传入TaskLifecycleListener，实际上是对两种实现类的包装。


##### QueryOutputTaskLifecycleListener

当父Stage为空（没有下游Stage）时，会绑定QueryOutputTaskLifecycleListener作为TaskLifecycleListener。

```java
private static class QueryOutputTaskLifecycleListener
            implements TaskLifecycleListener
    {
        private final QueryStateMachine queryStateMachine;

        private QueryOutputTaskLifecycleListener(QueryStateMachine queryStateMachine)
        {
            this.queryStateMachine = requireNonNull(queryStateMachine, "queryStateMachine is null");
        }

        @Override
        public void taskCreated(PlanFragmentId fragmentId, RemoteTask task)
        {
            URI taskUri = uriBuilderFrom(task.getTaskStatus().getSelf())
                    .appendPath("results")
                    .appendPath("0").build();
            DirectExchangeInput input = new DirectExchangeInput(task.getTaskId(), taskUri.toString());
            //更新查询结果的输入，将任务输出添加到输入队列中
            queryStateMachine.updateInputsForQueryResults(ImmutableList.of(input), false);
        }

        @Override
        public void noMoreTasks(PlanFragmentId fragmentId)
        {
            //更新查询结果的输入，第二个参数为true，表示已无更多输入
            queryStateMachine.updateInputsForQueryResults(ImmutableList.of(), true);
        }
    }
```



##### PipelinedStageExecution.getTaskLifecycleListener()

当没有开启重启策略，且存在父Stage时，会调用下游Stage的PipelinedStageExecution.getTaskLifecycleListener()获取TaskLifecycleListener，并作为构造参数传入。

```java
    public TaskLifecycleListener getTaskLifecycleListener()
    {
        return new TaskLifecycleListener()
        {
            @Override
            public void taskCreated(PlanFragmentId fragmentId, RemoteTask task)
            {
                sourceTaskCreated(fragmentId, task);
            }

            @Override
            public void noMoreTasks(PlanFragmentId fragmentId)
            {
                noMoreSourceTasks(fragmentId);
            }
        };
    }


    //当上游Task创建时，会调用该方法，更新当前Stage的SourceTask列表
    //并遍历当前Stage的所有Task，创建ExchangeSplit，添加到对应Task的Split列表中
    private synchronized void sourceTaskCreated(PlanFragmentId fragmentId, RemoteTask sourceTask)
    {
        requireNonNull(fragmentId, "fragmentId is null");

        //exchangeSources为当前Stage中的RemoteSourceNode
        RemoteSourceNode remoteSource = exchangeSources.get(fragmentId);
        checkArgument(remoteSource != null, "Unknown remote source %s. Known sources are %s", fragmentId, exchangeSources.keySet());

        //添加Source Task
        sourceTasks.put(fragmentId, sourceTask);

        //获取Source Task的outputBuffer，添加到source Task的OutputBuffers中
        PipelinedOutputBufferManager outputBufferManager = outputBufferManagers.get(fragmentId);
        sourceTask.setOutputBuffers(outputBufferManager.getOutputBuffers());

        //遍历当前Stage的所有Task,根据Source Task创建split，添加到Task的Split列表中
        for (RemoteTask destinationTask : getAllTasks()) {
            destinationTask.addSplits(ImmutableMultimap.of(remoteSource.getId(), createExchangeSplit(sourceTask, destinationTask)));
        }
    }

    private static Split createExchangeSplit(RemoteTask sourceTask, RemoteTask destinationTask)
    {
        //根据id从分配给Task的OutputBuffer中获取结果
        URI exchangeLocation = sourceTask.getTaskStatus().getSelf();
        //获取Source Task的split url地址
        URI splitLocation = uriBuilderFrom(exchangeLocation).appendPath("results").appendPath(String.valueOf(destinationTask.getTaskId().getPartitionId())).build();
        //根据Source Task的TaskId和url地址创建Split
        return new Split(REMOTE_CATALOG_HANDLE, new RemoteSplit(new DirectExchangeInput(sourceTask.getTaskId(), splitLocation.toString())));
    }

    private synchronized void noMoreSourceTasks(PlanFragmentId fragmentId)
    {
        RemoteSourceNode remoteSource = exchangeSources.get(fragmentId);
        checkArgument(remoteSource != null, "Unknown remote source %s. Known sources are %s", fragmentId, exchangeSources.keySet());

        //添加执行完成的上游Stage的fragmentId
        completeSourceFragments.add(fragmentId);

        //如果RemoteSourceNode对应的上游Stage都执行完成，将其添加到completeSources中，并通知所有Task上游Stage执行完成
        if (completeSourceFragments.containsAll(remoteSource.getSourceFragmentIds())) {
            completeSources.add(remoteSource.getId());
            for (RemoteTask task : getAllTasks()) {
                task.noMoreSplits(remoteSource.getId());
            }
        }
    }
```



##### PipelinedQueryScheduler.TaskLifecycleListenerBridge
PipelinedQueryScheduler.TaskLifecycleListenerBridge实际上是调用构造参数传入的TaskLifecycleListener，也就是调用的是QueryOutputTaskLifecycleListener或者PipelinedStageExecution.getTaskLifecycleListener()；如果父Stage为空（没有下游Stage）时，调用的是QueryOutputTaskLifecycleListener，否则调用的是PipelinedStageExecution.getTaskLifecycleListener()。
区别在于TaskLifecycleListenerBridge中多了notifyNoMoreSourceTasks()，在DistributedStagesScheduler.create()使用了该方法，用于在查询完成时关闭coordinator上的exchange clients。

```java
     private static class TaskLifecycleListenerBridge
            implements TaskLifecycleListener
    {
        private final TaskLifecycleListener listener;

        @GuardedBy("this")
        private final Set<PlanFragmentId> noMoreSourceTasks = new HashSet<>();
        @GuardedBy("this")
        private boolean done;

        private TaskLifecycleListenerBridge(TaskLifecycleListener listener)
        {
            this.listener = requireNonNull(listener, "listener is null");
        }

        @Override
        public synchronized void taskCreated(PlanFragmentId fragmentId, RemoteTask task)
        {
            checkState(!done, "unexpected state");
            listener.taskCreated(fragmentId, task);
        }

        @Override
        public synchronized void noMoreTasks(PlanFragmentId fragmentId)
        {
            checkState(!done, "unexpected state");
            noMoreSourceTasks.add(fragmentId);
        }

        public synchronized void notifyNoMoreSourceTasks()
        {
            checkState(!done, "unexpected state");
            done = true;
            noMoreSourceTasks.forEach(listener::noMoreTasks);
        }
    }
```


### StageScheduler

#### SourcePartitionedScheduler

SourcePartitionedScheduler负责Source Stage调度，它实现了SourceScheduler接口，可用于未分组或分组的pipeline，但需要调用方负责相应地初始化驱动程序组。

SourcePartitionedScheduler的start() 通过立即调度收集动态筛选器的任务来避免死锁，因为在流水线执行中，动态过滤器和join操作在同一个Stage被采集。这可能会导致在生成split之前等待动态筛选器的source stage join和连接器出现死锁（当动态筛选器至少需要一个probe task才能收集时，probe splits可能会被阻塞）。为了克服这个问题，为运行broadcast join操作的source stages创建初始任务。此任务允许在不安排任何probe splits的情况下进行动态筛选器收集。

##### SourcePartitionedScheduler启动执行
```java
    public synchronized void start()
    {
        //当动态过滤器需要采集任务时，才提前调度任务
        if (dynamicFilterService.isCollectingTaskNeeded(stageExecution.getStageId().getQueryId(), stageExecution.getFragment())) {
            //将Stage状态更新为Scheduling
            stageExecution.beginScheduling();
            //随机选择一个节点创建执行任务
            createTaskOnRandomNode();
        }
    }
    
    private void createTaskOnRandomNode()
    {
        checkState(scheduledTasks.isEmpty(), "Stage task is already scheduled on node");
        List<InternalNode> allNodes = splitPlacementPolicy.allNodes();
        checkState(allNodes.size() > 0, "No nodes available");
        //随机选择一个节点
        InternalNode node = allNodes.get(ThreadLocalRandom.current().nextInt(0, allNodes.size()));
        //在该Node上启动一个Task运行,所传split为空
        scheduleTask(node, ImmutableMultimap.of());
    }

```

##### SourcePartitionedScheduler调度执行

Source Task执行逻辑如下：

1. 根据split放置策略，生成Node与Split的对应关系；

2. 根据Node与Split的对应关系，在指定Node上启动Source Task处理位于该Node上的Splits。

```java
@Override
public synchronized ScheduleResult schedule()
{
     ...
       
    if (blockedFuture.isEmpty() && !pendingSplits.isEmpty()) {
        if (!placementFuture.isDone()) {
            blockedFuture = Optional.of(placementFuture);
            blockedOnPlacements = true;
        }
        else {
            //计算split的位置
            SplitPlacementResult splitPlacementResult = splitPlacementPolicy.computeAssignments(pendingSplits);
            //splitAssignment为Multimap<InternalNode, Split>类型，存放Node与Split的对应关系，记录了每个Node上存在哪些split
            splitAssignment = splitPlacementResult.getAssignments();
            //移除成功放置的split
            splitAssignment.values().forEach(pendingSplits::remove);
            overallSplitAssignmentCount += splitAssignment.size(); 
             //如果没有完成split放置，标记scheduleGroup为Blocked
            if (!pendingSplits.isEmpty()) {
                placementFuture = splitPlacementResult.getBlocked();
                blockedFuture = Optional.of(placementFuture);
                blockedOnPlacements = true;
            }
        }
    }

    ...

    //分配成功放置的split，完成Source Task的启动和执行
    overallNewTasks.addAll(assignSplits(splitAssignment));

    // 如果不分配新的split，则更新状态并绑定完成事件
    if (pendingSplits.isEmpty() && state == State.SPLITS_SCHEDULED) {
        log.debug("stage id: %s, node: %s; transitioning to FINISHED", stageExecution.getStageId(), partitionedNode);
        state = State.FINISHED;

        ...

        return new ScheduleResult(
                true,
                overallNewTasks.build(),
                overallSplitAssignmentCount);
    }

    if (blockedFuture.isEmpty()) {
        log.debug("stage id: %s, node: %s; assigned %s splits (not blocked)", stageExecution.getStageId(), partitionedNode, overallSplitAssignmentCount);
        return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);
    }

    //处理阻塞任务
    if (anySourceTaskBlocked.getAsBoolean()) {
        ...
    }

    ScheduleResult.BlockedReason blockedReason = blockedOnNextSplitBatch ? WAITING_FOR_SOURCE : SPLIT_QUEUES_FULL;
    log.debug("stage id: %s, node: %s; assigned %s splits (blocked reason %s)", stageExecution.getStageId(), partitionedNode, overallSplitAssignmentCount, blockedReason);
    return new ScheduleResult(
            false,
            overallNewTasks.build(),
            nonCancellationPropagating(blockedFuture.get()),
            blockedReason,
            overallSplitAssignmentCount);
}
```

assignSplits()方法对传入的splitAssignment进行遍历，每个Node执行下面操作：

1. 根据Node获取该Node上的Task
2. 若Task为空，则根据Node和splits创建一个新的任务，否则将该Node上的Splits提交给运行在该Node上的Task进行处理。

```java
		private Set<RemoteTask> assignSplits(Multimap<InternalNode, Split> splitAssignment)
    {
        ImmutableSet.Builder<RemoteTask> newTasks = ImmutableSet.builder();

        ImmutableSet<InternalNode> nodes = ImmutableSet.copyOf(splitAssignment.keySet());
        for (InternalNode node : nodes) {
            ImmutableMultimap<PlanNodeId, Split> splits = ImmutableMultimap.<PlanNodeId, Split>builder()
                    .putAll(partitionedNode, splitAssignment.get(node))
                    .build();
            //如果任务已创建，则直接从scheduledTasks中获取
            RemoteTask task = scheduledTasks.get(node);
            //task不为空,将当前Node上的Splits更新到运行在该Node的Task中需要处理的Split列表汇总
            if (task != null) {
                task.addSplits(splits);
            }
            else {
                //该Node没有启动Task,在该Node上启动一个Task运行，该Task用于处理splits
                scheduleTask(node, splits).ifPresent(newTasks::add);
            }
        }
        return newTasks.build();
    }
```

新任务的创建是由scheduleTask()完成的，该方法在指定的Node上启动一个Task并执行。实际调用的是PipelinedStageExecution的scheduleTask()。

```java
private Optional<RemoteTask> scheduleTask(InternalNode node, Multimap<PlanNodeId, Split> initialSplits)
{
    Optional<RemoteTask> remoteTask = stageExecution.scheduleTask(node, partitionIdAllocator.getNextId(), initialSplits);
    remoteTask.ifPresent(task -> scheduledTasks.put(node, task));
    return remoteTask;
}
```

####  FixedSourcePartitionedScheduler

##### FixedSourcePartitionedScheduler创建
FixedSourcePartitionedScheduler创建时，会为每个splitSource创建一个SourcePartitionedScheduler，由其负责splitSource的调度。

```java
 public FixedSourcePartitionedScheduler(
            StageExecution stageExecution,
            Map<PlanNodeId, SplitSource> splitSources,
            List<PlanNodeId> schedulingOrder,
            List<InternalNode> nodes,
            BucketNodeMap bucketNodeMap,
            int splitBatchSize,
            NodeSelector nodeSelector,
            DynamicFilterService dynamicFilterService,
            TableExecuteContextManager tableExecuteContextManager)
    {
        ...

        //使用内部类FixedSourcePartitionedScheduler.BucketedSplitPlacementPolicy作为SourceScheduler的SplitPlacementPolicy
        BucketedSplitPlacementPolicy splitPlacementPolicy = new BucketedSplitPlacementPolicy(nodeSelector, nodes, bucketNodeMap, stageExecution::getAllTasks);

        ArrayList<SourceScheduler> sourceSchedulers = new ArrayList<>();

        partitionIdAllocator = new PartitionIdAllocator();
        scheduledTasks = new HashMap<>();
	//遍历所有Source,每个Source创建一个SourcePartitionedScheduler，添加到sourceSchedulers中
        for (PlanNodeId planNodeId : schedulingOrder) {
            SplitSource splitSource = splitSources.get(planNodeId);
            SourceScheduler sourceScheduler = newSourcePartitionedSchedulerAsSourceScheduler(
                    stageExecution,
                    planNodeId,
                    splitSource,
                    splitPlacementPolicy,
                    splitBatchSize,
                    dynamicFilterService,
                    tableExecuteContextManager,
                    () -> true,
                    partitionIdAllocator,
                    scheduledTasks);

            sourceSchedulers.add(sourceScheduler);
        }
        this.sourceSchedulers = new ArrayDeque<>(sourceSchedulers);
    }
```

##### FixedSourcePartitionedScheduler调度执行
FixedSourcePartitionedScheduler调度执行步骤如下：
1.如果已调度Task为空，则创建Task，并将其存放到newTasks列表中
2.遍历所有的SourcePartitionedScheduler，依次执行其schedule()方法进行调度，调度完成后从sourceSchedulers中移除SourcePartitionedScheduler
3.返回调度结果，调度结果包括是否调度完成，newTasks列表和调度完成的Split数量，如果存在阻塞的Split，还会返回阻塞原因和阻塞的split

```java
    public ScheduleResult schedule()
    {
        // schedule a task on every node in the distribution
        List<RemoteTask> newTasks = ImmutableList.of();
	
	//当scheduledTasks为空时，每个Node创建一个Task
        if (scheduledTasks.isEmpty()) {
            ImmutableList.Builder<RemoteTask> newTasksBuilder = ImmutableList.builder();
            for (InternalNode node : nodes) {
                Optional<RemoteTask> task = stageExecution.scheduleTask(node, partitionIdAllocator.getNextId(), ImmutableMultimap.of());
                if (task.isPresent()) {
		    //创建的任务存放在scheduledTasks，作为SourceScheduler构造参数传入，SourceScheduler.schedule()执行时，相同Node上不会重复创建Task
                    scheduledTasks.put(node, task.get());
                    newTasksBuilder.add(task.get());
                }
            }
            newTasks = newTasksBuilder.build();
        }

        ...
	//遍历所有的sourceScheduler，依次执行其schedule()方法进行调度
        while (!sourceSchedulers.isEmpty()) {
            SourceScheduler scheduler = sourceSchedulers.peek();
            ScheduleResult schedule = scheduler.schedule();
            splitsScheduled += schedule.getSplitsScheduled();
            blocked = schedule.getBlocked();

            if (schedule.getBlockedReason().isPresent()) {
                blockedReason = schedule.getBlockedReason().get();
            }
            else {
                blockedReason = null;
            }

            // if the source is not done scheduling, stop scheduling for now
            if (!blocked.isDone() || !schedule.isFinished()) {
                break;
            }

            stageExecution.schedulingComplete(scheduler.getPlanNodeId());
            sourceSchedulers.remove().close();
        }

        if (blockedReason != null) {
            return new ScheduleResult(sourceSchedulers.isEmpty(), newTasks, blocked, blockedReason, splitsScheduled);
        }
        checkState(blocked.isDone(), "blockedReason not provided when scheduler is blocked");
        return new ScheduleResult(sourceSchedulers.isEmpty(), newTasks, splitsScheduled);
    }
```


#### FixedCountScheduler

FixedCountScheduler直接在构造方法中的传入的每个Node上创建执行任务，由PipelinedStageExecution负责Task的调度。

```java
    public FixedCountScheduler(StageExecution stageExecution, List<InternalNode> partitionToNode)
    {
        requireNonNull(stageExecution, "stage is null");
        this.taskScheduler = (node, partition) -> stageExecution.scheduleTask(node, partition, ImmutableMultimap.of());
        this.partitionToNode = requireNonNull(partitionToNode, "partitionToNode is null");
    }

    public ScheduleResult schedule()
    {
    	//在每个Node上创建一个Task
        List<RemoteTask> newTasks = IntStream.range(0, partitionToNode.size())
                .mapToObj(partition -> taskScheduler.scheduleTask(partitionToNode.get(partition), partition))
                .filter(Optional::isPresent)
                .map(Optional::get)
                .collect(toImmutableList());

        return new ScheduleResult(true, newTasks, 0);
    }
```


#### ScaledWriterScheduler
ScaledWriterScheduler允许 Trino 动态地扩展writer Task的数量，当写入遇到瓶颈，且每个writer的平均物理数据量超过最小阈值时，将添加新的任务。
当scale-writers配置开启时，会使用ScaledWriterScheduler负责写操作的Stage调度，否则使用FixedCountScheduler。

ScaledWriterScheduler调度执行的步骤如下：
1.获取可创建的Task数量
2.如果可创建Task数量不为0，则随机选择Node运行Task

```java
    public ScheduleResult schedule()
    {
        List<RemoteTask> writers = scheduleTasks(getNewTaskCount());

        future.set(null);
        future = SettableFuture.create();
        executor.schedule(() -> future.set(null), 200, MILLISECONDS);

        return new ScheduleResult(done.get(), writers, future, WRITER_SCALING, 0);
    }
    
    //获取新增Task数量
    private int getNewTaskCount()
    {
        //已调度Node为空时，新增一个Task
        if (scheduledNodes.isEmpty()) {
            return 1;
        }

	//计算当前Stage所有写入Writer Task已写入数据大小总和
        long writtenBytes = writerTasksProvider.get().stream()
                .map(TaskStatus::getPhysicalWrittenDataSize)
                .mapToLong(DataSize::toBytes)
                .sum();

        // When there is a big data skewness, there could be a bottleneck due to the skewed workers even if most of the workers are not over-utilized.
        // Check both, weighted output buffer over-utilization rate and average output buffer over-utilization rate, in case when there are many over-utilized small tasks
        // due to fewer not-over-utilized big skewed tasks.
        if ((isWeightedBufferFull() || isAverageBufferFull()) && (writtenBytes >= (writerMinSizeBytes * maxTaskWriterCount * scheduledNodes.size()))) {
            return 1;
        }

        return 0;
    }
    
    private List<RemoteTask> scheduleTasks(int count)
    {
        if (count == 0) {
            return ImmutableList.of();
        }

        //为新增Task随机选择执行Node，已执行Task的Node会被排除，由此每个Node节点只会运行一个当前Stage的Task
        List<InternalNode> nodes = nodeSelector.selectRandomNodes(count, scheduledNodes);

        checkCondition(!scheduledNodes.isEmpty() || !nodes.isEmpty(), NO_NODES_AVAILABLE, "No nodes available to run query");

        ImmutableList.Builder<RemoteTask> tasks = ImmutableList.builder();
        for (InternalNode node : nodes) {
            //在对应Node上创建并执行任务
            Optional<RemoteTask> remoteTask = stage.scheduleTask(node, scheduledNodes.size(), ImmutableMultimap.of());
            //任务创建后，添加到tasks，并将创建了Task的Node加入到scheduledNodes中
            remoteTask.ifPresent(task -> {
                tasks.add(task);
                scheduledNodes.add(node);
            });
        }

        return tasks.build();
    }
```

## EventDrivenFaultTolerantQueryScheduler
当重试策略为Task时，EventDrivenFaultTolerantQueryScheduler负责查询调度。


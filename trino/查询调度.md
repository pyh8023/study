Trino根据生成的逻辑执行计划将其拆分成多个具有层级关系的Stage，每个Stage会进一步分解为若干个Task，并将Task调度到不同的Worker节点上执行，每个的Stage有不同的调度策略。

查询调度的入口在SqlQueryExecution.start()中，根据不同的重试策略选择QueryScheduler实现类：

```
private void planDistribution(PlanRoot plan)
    {
        // if query was canceled, skip creating scheduler
        if (stateMachine.isDone()) {
            return;
        }

        // record output field
        PlanFragment rootFragment = plan.getRoot().getFragment();
        stateMachine.setColumns(
                ((OutputNode) rootFragment.getRoot()).getColumnNames(),
                rootFragment.getTypes());

        RetryPolicy retryPolicy = getRetryPolicy(getSession());
        QueryScheduler scheduler;
        switch (retryPolicy) {
            case QUERY:
            case NONE:
                scheduler = new PipelinedQueryScheduler(
                        stateMachine,
                        plan.getRoot(),
                        nodePartitioningManager,
                        nodeScheduler,
                        remoteTaskFactory,
                        plan.isSummarizeTaskInfos(),
                        scheduleSplitBatchSize,
                        queryExecutor,
                        schedulerExecutor,
                        failureDetector,
                        nodeTaskMap,
                        executionPolicy,
                        schedulerStats,
                        dynamicFilterService,
                        tableExecuteContextManager,
                        plannerContext.getMetadata(),
                        splitSourceFactory,
                        coordinatorTaskManager);
                break;
            case TASK:
                if (isFaultTolerantExecutionEventDriverSchedulerEnabled(stateMachine.getSession())) {
                    scheduler = new EventDrivenFaultTolerantQueryScheduler(
                            stateMachine,
                            plannerContext.getMetadata(),
                            remoteTaskFactory,
                            taskDescriptorStorage,
                            eventDrivenTaskSourceFactory,
                            plan.isSummarizeTaskInfos(),
                            nodeTaskMap,
                            queryExecutor,
                            schedulerExecutor,
                            schedulerStats,
                            partitionMemoryEstimatorFactory,
                            nodePartitioningManager,
                            exchangeManagerRegistry.getExchangeManager(),
                            nodeAllocatorService,
                            failureDetector,
                            dynamicFilterService,
                            taskExecutionStats,
                            plan.getRoot());
                }
                else {
                    scheduler = new FaultTolerantQueryScheduler(
                            stateMachine,
                            queryExecutor,
                            schedulerStats,
                            failureDetector,
                            taskSourceFactory,
                            taskDescriptorStorage,
                            exchangeManagerRegistry.getExchangeManager(),
                            nodePartitioningManager,
                            getTaskRetryAttemptsOverall(getSession()),
                            getTaskRetryAttemptsPerTask(getSession()),
                            getMaxTasksWaitingForNodePerStage(getSession()),
                            schedulerExecutor,
                            nodeAllocatorService,
                            partitionMemoryEstimatorFactory,
                            taskExecutionStats,
                            dynamicFilterService,
                            plannerContext.getMetadata(),
                            remoteTaskFactory,
                            nodeTaskMap,
                            plan.getRoot(),
                            plan.isSummarizeTaskInfos());
                }
                break;
            default:
                throw new IllegalArgumentException("Unexpected retry policy: " + retryPolicy);
        }

        queryScheduler.set(scheduler);
    }
```

然后调用QueryScheduler的start()方法启动查询调度。

当重试策略为NONE或QUERY时，采用PipelinedQueryScheduler进行调度，重试策略为TASK时，采用EventDrivenFaultTolerantQueryScheduler和FaultTolerantQueryScheduler，其中FaultTolerantQueryScheduler已过时，已被EventDrivenFaultTolerantQueryScheduler取代，下面主要对PipelinedQueryScheduler和EventDrivenFaultTolerantQueryScheduler进行分析。



# QueryScheduler

## PipelinedQueryScheduler
PipelinedQueryScheduler的大致调度过程如下图所示：

![image-pipelinedQueryScheduler](../trino/images/image-pipelinedQueryScheduler.png)

在PipelinedQueryScheduler中创建PipelinedQueryScheduler.CoordinatorStagesScheduler 和PipelinedQueryScheduler.DistributedStagesScheduler两个Stage调度器：
PipelinedQueryScheduler.CoordinatorStagesScheduler负责Coordinator_Only类型的Stage调度
PipelinedQueryScheduler.DistributedStagesScheduler负责Coordinator_Only类型之外的所有Stage的调度


DistributedStagesScheduler.create()

public static DistributedStagesScheduler create(
        QueryStateMachine queryStateMachine,
        SplitSchedulerStats schedulerStats,
        NodeScheduler nodeScheduler,
        NodePartitioningManager nodePartitioningManager,
        StageManager stageManager,
        CoordinatorStagesScheduler coordinatorStagesScheduler,
        ExecutionPolicy executionPolicy,
        FailureDetector failureDetector,
        ScheduledExecutorService executor,
        SplitSourceFactory splitSourceFactory,
        int splitBatchSize,
        DynamicFilterService dynamicFilterService,
        TableExecuteContextManager tableExecuteContextManager,
        RetryPolicy retryPolicy,
        int attempt)
{
    DistributedStagesSchedulerStateMachine stateMachine = new DistributedStagesSchedulerStateMachine(queryStateMachine.getQueryId(), executor);

    Map<PartitioningHandle, NodePartitionMap> partitioningCacheMap = new HashMap<>();
    Function<PartitioningHandle, NodePartitionMap> partitioningCache = partitioningHandle ->
            partitioningCacheMap.computeIfAbsent(partitioningHandle, handle -> nodePartitioningManager.getNodePartitioningMap(
                    queryStateMachine.getSession(),
                    // TODO: support hash distributed writer scaling (https://github.com/trinodb/trino/issues/10791)
                    handle.equals(SCALED_WRITER_HASH_DISTRIBUTION) ? FIXED_HASH_DISTRIBUTION : handle));

    //为每一个Stage创建bucketToPartition
    Map<PlanFragmentId, Optional<int[]>> bucketToPartitionMap = createBucketToPartitionMap(
            coordinatorStagesScheduler.getBucketToPartitionForStagesConsumedByCoordinator(),
            stageManager,
            partitioningCache);
    //为每一个Stage创建PipelinedOutputBufferManager，根据Stage对应PlanFragment的Partitioning类型，选择不同的PipelinedOutputBufferManager实现类
    //FIXED_BROADCAST_DISTRIBUTION类型选择BroadcastPipelinedOutputBufferManager
    //SCALED_WRITER_ROUND_ROBIN_DISTRIBUTION选择ScaledPipelinedOutputBufferManager
    //其他类型选择PartitionedPipelinedOutputBufferManager
    Map<PlanFragmentId, PipelinedOutputBufferManager> outputBufferManagers = createOutputBufferManagers(
            coordinatorStagesScheduler.getOutputBuffersForStagesConsumedByCoordinator(),
            stageManager,
            bucketToPartitionMap);

    TaskLifecycleListener coordinatorTaskLifecycleListener = coordinatorStagesScheduler.getTaskLifecycleListener();
    
    //当开启重启机制，仅在查询完成时关闭coordinator上的exchange clients
    if (retryPolicy != RetryPolicy.NONE) {
        TaskLifecycleListenerBridge taskLifecycleListenerBridge = new TaskLifecycleListenerBridge(coordinatorTaskLifecycleListener);
        coordinatorTaskLifecycleListener = taskLifecycleListenerBridge;
        stateMachine.addStateChangeListener(state -> {
            if (state == DistributedStagesSchedulerState.FINISHED) {
                taskLifecycleListenerBridge.notifyNoMoreSourceTasks();
            }
        });
    }

    Map<StageId, StageExecution> stageExecutions = new HashMap<>();
    for (SqlStage stage : stageManager.getDistributedStagesInTopologicalOrder()) {
        Optional<SqlStage> parentStage = stageManager.getParent(stage.getStageId());
        
        //获取任务生命周期监听器
        TaskLifecycleListener taskLifecycleListener;
        if (parentStage.isEmpty() || parentStage.get().getFragment().getPartitioning().isCoordinatorOnly()) {
            // output will be consumed by coordinator
            taskLifecycleListener = coordinatorTaskLifecycleListener;
        }
        else {
            StageId parentStageId = parentStage.get().getStageId();
            StageExecution parentStageExecution = requireNonNull(stageExecutions.get(parentStageId), () -> "execution is null for stage: " + parentStageId);
            taskLifecycleListener = parentStageExecution.getTaskLifecycleListener();
        }

        PlanFragment fragment = stage.getFragment();
        //创建PipelinedStageExecution
        StageExecution stageExecution = createPipelinedStageExecution(
                stageManager.get(fragment.getId()),
                outputBufferManagers,
                taskLifecycleListener,
                failureDetector,
                executor,
                bucketToPartitionMap.get(fragment.getId()),
                attempt);
        stageExecutions.put(stage.getStageId(), stageExecution);
    }

    ImmutableMap.Builder<StageId, StageScheduler> stageSchedulers = ImmutableMap.builder();
    for (StageExecution stageExecution : stageExecutions.values()) {
        List<StageExecution> children = stageManager.getChildren(stageExecution.getStageId()).stream()
                .map(stage -> requireNonNull(stageExecutions.get(stage.getStageId()), () -> "stage execution not found for stage: " + stage))
                .collect(toImmutableList());
                
        //根据Stage对应PlanFragment的Partitioning类型，选择不同的StageScheduler实现类
        StageScheduler scheduler = createStageScheduler(
                queryStateMachine,
                stageExecution,
                splitSourceFactory,
                children,
                partitioningCache,
                nodeScheduler,
                nodePartitioningManager,
                splitBatchSize,
                dynamicFilterService,
                executor,
                tableExecuteContextManager);
        stageSchedulers.put(stageExecution.getStageId(), scheduler);
    }

    DistributedStagesScheduler distributedStagesScheduler = new DistributedStagesScheduler(
            stateMachine,
            queryStateMachine,
            schedulerStats,
            stageManager,
            executionPolicy.createExecutionSchedule(stageExecutions.values()),
            stageSchedulers.buildOrThrow(),
            ImmutableMap.copyOf(stageExecutions),
            dynamicFilterService);
    
    //为每个StageExecution设置状态变更监听器
    distributedStagesScheduler.initialize();
    return distributedStagesScheduler;
}


private static StageScheduler createStageScheduler(
        QueryStateMachine queryStateMachine,
        StageExecution stageExecution,
        SplitSourceFactory splitSourceFactory,
        List<StageExecution> childStageExecutions,
        Function<PartitioningHandle, NodePartitionMap> partitioningCache,
        NodeScheduler nodeScheduler,
        NodePartitioningManager nodePartitioningManager,
        int splitBatchSize,
        DynamicFilterService dynamicFilterService,
        ScheduledExecutorService executor,
        TableExecuteContextManager tableExecuteContextManager)
{
    Session session = queryStateMachine.getSession();
    PlanFragment fragment = stageExecution.getFragment();
    
    //获取PlanFragment的Partitioning类型
    PartitioningHandle partitioningHandle = fragment.getPartitioning();
    
    //获取数据源split，只有带有TableScanNode的Stage，splitSources不为空
    Map<PlanNodeId, SplitSource> splitSources = splitSourceFactory.createSplitSources(session, fragment);
    if (!splitSources.isEmpty()) {
        queryStateMachine.addStateChangeListener(new StateChangeListener<>()
        {
            private final AtomicReference<Collection<SplitSource>> splitSourcesReference = new AtomicReference<>(splitSources.values());

            @Override
            public void stateChanged(QueryState newState)
            {
                if (newState.isDone()) {
                    // ensure split sources are closed and release memory
                    Collection<SplitSource> sources = splitSourcesReference.getAndSet(null);
                    if (sources != null) {
                        closeSplitSources(sources);
                    }
                }
            }
        });
    }

    if (partitioningHandle.equals(SOURCE_DISTRIBUTION)) {
        // nodes are selected dynamically based on the constraints of the splits and the system load
        Entry<PlanNodeId, SplitSource> entry = getOnlyElement(splitSources.entrySet());
        PlanNodeId planNodeId = entry.getKey();
        SplitSource splitSource = entry.getValue();
        Optional<CatalogHandle> catalogHandle = Optional.of(splitSource.getCatalogHandle())
                .filter(catalog -> !catalog.getType().isInternal());
        NodeSelector nodeSelector = nodeScheduler.createNodeSelector(session, catalogHandle);
        SplitPlacementPolicy placementPolicy = new DynamicSplitPlacementPolicy(nodeSelector, stageExecution::getAllTasks);

        return newSourcePartitionedSchedulerAsStageScheduler(
                stageExecution,
                planNodeId,
                splitSource,
                placementPolicy,
                splitBatchSize,
                dynamicFilterService,
                tableExecuteContextManager,
                () -> childStageExecutions.stream().anyMatch(StageExecution::isAnyTaskBlocked));
    }

    if (partitioningHandle.equals(SCALED_WRITER_ROUND_ROBIN_DISTRIBUTION)) {
        Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStageExecutions.stream()
                .map(StageExecution::getTaskStatuses)
                .flatMap(List::stream)
                .collect(toImmutableList());
        Supplier<Collection<TaskStatus>> writerTasksProvider = stageExecution::getTaskStatuses;

        ScaledWriterScheduler scheduler = new ScaledWriterScheduler(
                stageExecution,
                sourceTasksProvider,
                writerTasksProvider,
                nodeScheduler.createNodeSelector(session, Optional.empty()),
                executor,
                getWriterMinSize(session),
                isTaskScaleWritersEnabled(session) ? getTaskScaleWritersMaxWriterCount(session) : getTaskWriterCount(session));

        whenAllStages(childStageExecutions, StageExecution.State::isDone)
                .addListener(scheduler::finish, directExecutor());

        return scheduler;
    }

    if (splitSources.isEmpty()) {
        // all sources are remote
        NodePartitionMap nodePartitionMap = partitioningCache.apply(partitioningHandle);
        //可运行Task的Node
        List<InternalNode> partitionToNode = nodePartitionMap.getPartitionToNode();
        // todo this should asynchronously wait a standard timeout period before failing
        checkCondition(!partitionToNode.isEmpty(), NO_NODES_AVAILABLE, "No worker nodes available");
        return new FixedCountScheduler(stageExecution, partitionToNode);
    }

    // contains local source
    List<PlanNodeId> schedulingOrder = fragment.getPartitionedSources();
    Optional<CatalogHandle> catalogHandle = partitioningHandle.getCatalogHandle();
    checkArgument(catalogHandle.isPresent(), "No catalog handle for partitioning handle: %s", partitioningHandle);

    BucketNodeMap bucketNodeMap;
    List<InternalNode> stageNodeList;
    if (fragment.getRemoteSourceNodes().stream().allMatch(node -> node.getExchangeType() == REPLICATE)) {
        // no remote source
        bucketNodeMap = nodePartitioningManager.getBucketNodeMap(session, partitioningHandle);
        stageNodeList = new ArrayList<>(nodeScheduler.createNodeSelector(session, catalogHandle).allNodes());
        Collections.shuffle(stageNodeList);
    }
    else {
        // remote source requires nodePartitionMap
        NodePartitionMap nodePartitionMap = partitioningCache.apply(partitioningHandle);
        stageNodeList = nodePartitionMap.getPartitionToNode();
        bucketNodeMap = nodePartitionMap.asBucketNodeMap();
    }

    return new FixedSourcePartitionedScheduler(
            stageExecution,
            splitSources,
            schedulingOrder,
            stageNodeList,
            bucketNodeMap,
            splitBatchSize,
            nodeScheduler.createNodeSelector(session, catalogHandle),
            dynamicFilterService,
            tableExecuteContextManager);
}
​





public static PipelinedStageExecution createPipelinedStageExecution(
        SqlStage stage,
        Map<PlanFragmentId, PipelinedOutputBufferManager> outputBufferManagers,
        TaskLifecycleListener taskLifecycleListener,
        FailureDetector failureDetector,
        Executor executor,
        Optional<int[]> bucketToPartition,
        int attempt)
{
    PipelinedStageStateMachine stateMachine = new PipelinedStageStateMachine(stage.getStageId(), executor);
    ImmutableMap.Builder<PlanFragmentId, RemoteSourceNode> exchangeSources = ImmutableMap.builder();
    for (RemoteSourceNode remoteSourceNode : stage.getFragment().getRemoteSourceNodes()) {
        for (PlanFragmentId planFragmentId : remoteSourceNode.getSourceFragmentIds()) {
            exchangeSources.put(planFragmentId, remoteSourceNode);
        }
    }
    PipelinedStageExecution execution = new PipelinedStageExecution(
            stateMachine,
            stage,
            outputBufferManagers,
            taskLifecycleListener,
            failureDetector,
            bucketToPartition,
            exchangeSources.buildOrThrow(),
            attempt);
    
    //设置Stage状态变更监听器
    execution.initialize();
    return execution;
}

## EventDrivenFaultTolerantQueryScheduler
